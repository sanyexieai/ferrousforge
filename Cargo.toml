[package]
name = "ferrousforge"
version = "0.1.0"
edition = "2021"
authors = ["FerrousForge Contributors"]
description = "A unified inference platform supporting multiple model types (text, image, audio, video) built with Rust"
license = "MIT OR Apache-2.0"
repository = "https://github.com/your-org/ferrousforge"
homepage = "https://github.com/your-org/ferrousforge"
documentation = "https://docs.rs/ferrousforge"
keywords = ["ai", "ml", "inference", "llm", "model", "ollama"]
categories = ["science", "web-programming", "api-bindings"]

[workspace]
members = []

[dependencies]
# 异步运行时
tokio = { version = "1.35", features = ["full"] }
async-trait = "0.1"

# Web 框架
axum = { version = "0.7", features = ["macros"] }
tower = "0.4"
tower-http = { version = "0.5", features = ["cors", "trace", "compression-gzip"] }

# 序列化
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
toml = "0.8"

# 配置管理
config = "0.14"

# 日志和追踪
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }

# 错误处理
thiserror = "2.0"
anyhow = "1.0"

# HTTP 客户端（用于模型下载）
reqwest = { version = "0.11", features = ["json", "stream"] }

# 时间处理
chrono = { version = "0.4", features = ["serde"] }

# 文件系统
walkdir = "2.4"

# CLI
clap = { version = "4.4", features = ["derive"] }

# 流处理
futures = "0.3"
futures-util = "0.3"

# 并发
parking_lot = "0.12"
num_cpus = "1.16"

# 指标和监控（可选，未来使用）
# prometheus = "0.13"
# opentelemetry = "0.21"
# opentelemetry-otlp = "0.14"

# 模型相关（未来添加）
# candle-core = "0.3"
# candle-nn = "0.3"
# candle-transformers = "0.3"

# llama.cpp 绑定
# 选项1: 使用现有的 Rust 绑定库（推荐）
# 注意：如果该库不可用，可以使用动态加载方式
# llama_cpp_rs = { version = "0.1", optional = true }
# 选项2: 使用 libloading 来动态加载 llama.cpp 库（备选）
libloading = { version = "0.8", optional = true }
# 选项3: 使用 bindgen 生成绑定（需要 build.rs）
# bindgen = { version = "0.69", optional = true }

[build-dependencies]
# 用于生成 FFI 绑定（如果使用 bindgen）
# bindgen = { version = "0.69", optional = true }
# 用于编译 C/C++ 代码
cc = { version = "1.0", optional = true }
# 用于下载文件
reqwest = { version = "0.11", optional = true, default-features = false, features = ["blocking"] }
# 用于解压文件
flate2 = { version = "1.0", optional = true }
tar = { version = "0.4", optional = true }

[features]
default = []
# 启用 llama.cpp 支持（静态链接，需要系统安装 llama.cpp 库）
# 使用 build.rs 配置链接
llama-cpp = []
# 启用 llama.cpp 支持（动态加载，需要系统安装 llama.cpp）
llama-cpp-dynamic = ["libloading"]
# 启用 llama.cpp 支持（使用 bindgen 生成绑定）
# llama-cpp-bindgen = ["bindgen"]

[dev-dependencies]
# 测试
tokio-test = "0.4"
mockall = "0.12"

# 测试工具
tempfile = "3.8"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
panic = "abort"

[profile.dev]
opt-level = 0
debug = true

[lib]
name = "ferrousforge"
path = "src/lib.rs"

[[bin]]
name = "ferrousforge"
path = "src/main.rs"

# 集成测试
[[test]]
name = "integration_llama_cpp"
path = "tests/integration_llama_cpp.rs"

