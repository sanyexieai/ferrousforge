# FerrousForge 实现路线图

## 开发阶段规划

### 阶段 0: 项目初始化（MVP 前准备）

**目标**：搭建基础项目结构

**任务**：
- [x] 初始化 Cargo 项目
- [x] 配置基础依赖（tokio, serde, axum 等）
- [x] 创建目录结构
- [x] 设置 CI/CD 基础配置
- [x] 编写基础 README

**预计时间**：1-2 天

---

### 阶段 1: 核心基础设施（MVP 基础）

**目标**：实现核心抽象和基础功能

**任务**：
- [x] 实现配置系统（config/）
  - [x] 配置结构定义
  - [x] 配置文件加载
  - [x] 环境变量支持
  - [x] 默认配置

- [x] 实现日志系统（utils/logging.rs）
  - [x] tracing 集成
  - [x] 日志级别配置
  - [x] 日志格式化

- [x] 实现错误处理（api/error.rs）
  - [x] 错误类型定义
  - [x] 错误转换
  - [x] 错误响应格式化

- [x] 实现存储基础（storage/）
  - [x] 存储抽象 trait
  - [x] 文件系统存储实现
  - [x] 模型路径管理
  - [x] 模型清单管理

**预计时间**：1-2 周

---

### 阶段 2: 模型抽象层（核心抽象）

**目标**：定义和实现模型抽象接口

**任务**：
- [x] 实现模型 trait（models/traits.rs）
  - [x] `Model` trait
  - [x] `Inferable` trait
  - [x] `Streamable` trait
  - [x] 模型类型枚举

- [ ] 实现基础模型（models/base.rs）
  - [ ] 通用模型元数据
  - [ ] 模型生命周期管理
  - [ ] 模型状态管理

- [ ] 实现模型注册表（core/registry.rs）
  - [ ] 模型注册/注销
  - [ ] 模型查找
  - [ ] 模型元数据管理

**预计时间**：1 周

---

### 阶段 3: 第一个推理后端（文本模型 MVP）

**目标**：实现第一个可工作的推理后端和文本模型

**推荐后端选择**：
1. **Candle**（纯 Rust，优先推荐）
   - 优点：纯 Rust，易于集成，无 FFI
   - 缺点：性能可能不如 C++ 实现
   - 适合：快速原型，简单模型

2. **llama.cpp**（通过 FFI）
   - 优点：性能好，生态成熟
   - 缺点：需要 FFI，跨平台复杂
   - 适合：生产环境，复杂模型

**任务**：
- [ ] 实现推理后端抽象（inference/backend.rs）
  - [ ] `InferenceBackend` trait
  - [ ] 后端配置
  - [ ] 后端注册

- [ ] 实现第一个后端（推荐 Candle）
  - [ ] Candle 后端实现（inference/backends/candle.rs）
  - [ ] 模型加载
  - [ ] 推理执行
  - [ ] 资源管理

- [ ] 实现文本模型（models/text/）
  - [ ] 文本模型实现（models/text/model.rs）
  - [ ] 输入/输出处理
  - [ ] Token 化处理
  - [ ] 流式输出支持

- [ ] 实现核心引擎（core/engine.rs）
  - [ ] 引擎初始化
  - [ ] 模型加载管理
  - [ ] 推理请求处理
  - [ ] 资源管理

**预计时间**：2-3 周

---

### 阶段 4: HTTP API 服务器（API MVP）

**目标**：实现基础的 HTTP API 服务器

**任务**：
- [ ] 实现 HTTP 服务器（server/http/）
  - [ ] Axum 服务器设置
  - [ ] 路由定义
  - [ ] 中间件（日志、CORS）

- [ ] 实现 API 类型（api/）
  - [ ] 请求类型定义
  - [ ] 响应类型定义
  - [ ] 序列化/反序列化

- [ ] 实现请求处理器（server/http/handlers/）
  - [ ] 健康检查（health.rs）
  - [ ] 模型列表（models.rs）
  - [ ] 文本生成（generate.rs）
  - [ ] 错误处理

- [ ] 实现流式响应（server/http/streaming.rs）
  - [ ] Server-Sent Events (SSE)
  - [ ] 流式生成响应

**预计时间**：1-2 周

---

### 阶段 5: CLI 工具（用户体验）

**目标**：实现命令行工具

**任务**：
- [ ] 实现 CLI 框架（cli/）
  - [ ] 参数解析（clap）
  - [ ] 命令结构

- [ ] 实现核心命令
  - [ ] `serve` - 启动服务器
  - [ ] `list` - 列出模型
  - [ ] `run` - 运行模型（本地）
  - [ ] `pull` - 下载模型
  - [ ] `remove` - 删除模型

**预计时间**：1 周

---

### 阶段 6: 模型下载和管理（完整功能）

**目标**：实现模型下载和管理功能

**任务**：
- [ ] 实现模型下载（storage/download.rs）
  - [ ] HTTP 下载
  - [ ] 断点续传
  - [ ] 进度显示
  - [ ] 校验和验证

- [ ] 实现模型注册表（storage/registry.rs）
  - [ ] 模型清单格式（类似 Modelfile）
  - [ ] 模型元数据存储
  - [ ] 模型版本管理

- [ ] 实现缓存管理（storage/cache.rs）
  - [ ] LRU 缓存
  - [ ] 内存限制
  - [ ] 自动清理

**预计时间**：1-2 周

---

### 阶段 7: 任务调度和并发（性能优化）

**目标**：实现任务调度和并发控制

**任务**：
- [ ] 实现任务调度器（core/scheduler.rs）
  - [ ] 优先级队列
  - [ ] 任务分配
  - [ ] 并发控制

- [ ] 实现请求上下文（core/context.rs）
  - [ ] 上下文管理
  - [ ] 会话管理
  - [ ] 状态跟踪

- [ ] 性能优化
  - [ ] 批处理
  - [ ] 连接池
  - [ ] 内存优化

**预计时间**：1-2 周

---

### 阶段 8: 更多模型类型支持

**目标**：扩展支持图像、音频、视频模型

#### 8.1 图像模型支持

**任务**：
- [ ] 实现图像模型抽象（models/image/model.rs）
- [ ] 实现图像生成模型（models/image/generation.rs）
  - [ ] Stable Diffusion 支持
  - [ ] 输入/输出处理（图像编码/解码）
- [ ] 实现图像理解模型（models/image/vision.rs）
  - [ ] CLIP 支持
  - [ ] 图像分类

**预计时间**：2-3 周

#### 8.2 音频模型支持

**任务**：
- [ ] 实现音频模型抽象（models/audio/model.rs）
- [ ] 实现语音识别（models/audio/speech.rs）
  - [ ] Whisper 支持
  - [ ] 音频预处理
- [ ] 实现语音合成（可选）

**预计时间**：2-3 周

#### 8.3 视频模型支持

**任务**：
- [ ] 实现视频模型抽象（models/video/model.rs）
- [ ] 实现视频生成模型（models/video/generation.rs）
- [ ] 实现视频理解模型（models/video/understanding.rs）

**预计时间**：3-4 周

---

### 阶段 9: 更多推理后端

**目标**：支持更多推理后端选项

**任务**：
- [ ] llama.cpp 后端（inference/backends/llama_cpp.rs）
  - [ ] FFI 绑定
  - [ ] 模型加载
  - [ ] 推理执行

- [ ] ONNX Runtime 后端（inference/backends/onnx.rs）
  - [ ] ONNX Runtime 集成
  - [ ] 模型加载
  - [ ] 推理执行

- [ ] TensorRT 后端（可选，inference/backends/tensorrt.rs）
  - [ ] TensorRT 集成
  - [ ] GPU 加速

**预计时间**：每个后端 1-2 周

---

### 阶段 10: 高级功能

**目标**：实现高级功能和优化

**任务**：
- [ ] WebSocket 支持（server/websocket/）
  - [ ] WebSocket 服务器
  - [ ] 实时流式响应

- [ ] gRPC 支持（可选，server/grpc/）
  - [ ] Protocol Buffers 定义
  - [ ] gRPC 服务器实现

- [ ] 监控和指标（utils/metrics.rs）
  - [ ] Prometheus 集成
  - [ ] 关键指标收集

- [ ] 认证授权
  - [ ] API 密钥认证
  - [ ] JWT 支持（可选）

- [ ] 多租户支持（可选）
  - [ ] 用户隔离
  - [ ] 资源配额

**预计时间**：每个功能 1-2 周

---

## MVP（最小可行产品）定义

### MVP 范围

**核心功能**：
1. ✅ 文本模型加载和推理
2. ✅ HTTP API 服务器
3. ✅ 基础 CLI 工具
4. ✅ 模型下载和管理
5. ✅ 一个推理后端（Candle 或 llama.cpp）

**不包括**：
- ❌ 图像/音频/视频模型
- ❌ 多个推理后端
- ❌ WebSocket/gRPC
- ❌ 高级监控
- ❌ 认证授权

**MVP 目标**：能够运行一个文本模型，通过 HTTP API 进行推理

**预计完成时间**：阶段 1-5，约 6-8 周

---

## 技术债务和后续优化

### 短期优化（MVP 后）

- [ ] 代码重构和清理
- [ ] 文档完善
- [ ] 测试覆盖率提升
- [ ] 性能基准测试
- [ ] 错误处理改进

### 中期优化（3-6 个月）

- [ ] 模型量化支持
- [ ] GPU 加速优化
- [ ] 批处理优化
- [ ] 内存管理优化
- [ ] 分布式部署支持

### 长期规划（6-12 个月）

- [ ] 模型训练支持
- [ ] 模型微调支持
- [ ] 插件系统
- [ ] 云原生优化
- [ ] 多语言 SDK

---

## 开发建议

### 1. 迭代开发

- 每个阶段完成后进行测试和验证
- 保持代码可运行状态
- 及时重构和优化

### 2. 测试驱动

- 每个功能编写测试
- 保持测试覆盖率
- 集成测试和单元测试并重

### 3. 文档先行

- API 文档
- 架构文档
- 使用示例

### 4. 社区反馈

- 早期发布 MVP
- 收集用户反馈
- 根据反馈调整优先级

---

## 关键决策点

### 需要讨论的决策

1. **第一个后端选择**
   - Candle（纯 Rust，快速开发）
   - llama.cpp（性能好，但需要 FFI）

2. **模型格式支持**
   - GGUF（llama.cpp 格式）
   - Safetensors（HuggingFace）
   - ONNX
   - 原生格式

3. **API 兼容性**
   - 完全兼容 Ollama API
   - 自定义 API 设计
   - 混合方案

4. **部署方式优先级**
   - Docker 容器
   - 系统服务
   - 云原生（K8s）

5. **多模型类型优先级**
   - 先完善文本模型
   - 还是快速支持多种类型

---

## 里程碑

- **M1**: 核心基础设施完成（阶段 1-2）
- **M2**: 第一个可工作的文本模型（阶段 3）
- **M3**: HTTP API 可用（阶段 4）
- **M4**: MVP 完成（阶段 1-5）
- **M5**: 图像模型支持（阶段 8.1）
- **M6**: 多后端支持（阶段 9）
- **M7**: 生产就绪（阶段 10）

